import torch
import psutil

print("ğŸ”§ SYSTEM CHECK\n")

# CPU
cpu_usage = psutil.cpu_percent(interval=1)
print(f"ğŸ§  CPU Usage: {cpu_usage}%")

# RAM
ram = psutil.virtual_memory()
print(f"ğŸ’¾ RAM Total: {round(ram.total / (1024 ** 3), 2)} GB")
print(f"ğŸ’¾ RAM Used: {round(ram.used / (1024 ** 3), 2)} GB")
print(f"ğŸ’¾ RAM Usage: {ram.percent}%")

# GPU
print("\nğŸ® GPU INFO")
if torch.cuda.is_available():
    print("âœ… CUDA is available")
    print(f"ğŸ§® GPU Count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"\n  ğŸ”¹ GPU {i + 1}: {torch.cuda.get_device_name(i)}")
        props = torch.cuda.get_device_properties(i)
        print(f"     Total Memory: {round(props.total_memory / (1024 ** 3), 2)} GB")
        print(f"     Allocated:    {round(torch.cuda.memory_allocated(i) / (1024 ** 3), 2)} GB")
        print(f"     Reserved:     {round(torch.cuda.memory_reserved(i) / (1024 ** 3), 2)} GB")
else:
    print("âŒ CUDA is NOT available")
    print("   Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ñƒ Ğ²Ğ°Ñ Ğ½ĞµÑ‚ NVIDIA GPU Ğ¸Ğ»Ğ¸ Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° CUDA.")
